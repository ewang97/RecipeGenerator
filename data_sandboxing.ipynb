{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import splitfolders\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer   # for tokenization\n",
    "from collections import Counter     # for tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitfolders.ratio(\"Food Images\", output=\"food_images\", \n",
    "#                    seed=30, ratio=(.7, .2, .1), \n",
    "#                    group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crispy-salt-and-pepper-potatoes-dan-kluger'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('Food Ingredients and Recipe Dataset with Image Name Mapping.csv')\n",
    "data_df.iloc[1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './food_images'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                           [0.229,0.224,0.225])])\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(size=224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                        [0.229,0.224,0.225])])\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(size=224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                        [0.229,0.224,0.225])])\n",
    "\n",
    "# train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "# valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "# test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(train_data,batch_size = 64, shuffle=True)\n",
    "# validloader = torch.utils.data.DataLoader(valid_data,batch_size = 64, shuffle=True)\n",
    "# testloader = torch.utils.data.DataLoader(test_data,batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq = 1):\n",
    "        self.itos = {0:'<PAD>',1:'<START>',2:'<END>',3:'<UNK>'}\n",
    "        self.stoi = {'<PAD>':0,'<START>':1,'<END>':2,'<UNK>':3}\n",
    "\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.frequencies = Counter()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            sentence_tokens = self.tokenizer(str(sentence))\n",
    "            self.frequencies.update(sentence_tokens)\n",
    "            #TODO: handle null sentences/instructions\n",
    "            for token in sentence_tokens:\n",
    "                if token not in self.stoi.keys() and self.frequencies[token] >= self.min_freq:\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx +=1\n",
    "\n",
    "    def numericalize(self, sentence):\n",
    "        sentence_tokens = self.tokenizer(str(sentence))\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi['<UNK>'] for token in sentence_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        recipes = [item[1] for item in batch]\n",
    "        recipes = pad_sequence(recipes, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, recipe_csv, transform = None, min_freq = 3):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        init_df = pd.read_csv(recipe_csv)\n",
    "        ###TODO cleaning function :\n",
    "        # clean_df = init_df.dropna(subset=['Instructions'], inplace=True)\n",
    "        clean_df = init_df[init_df[\"Image_Name\"] != '#NAME?']\n",
    "        \n",
    "        self.recipe_df = clean_df\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "        self.vocab = Vocab(min_freq)\n",
    "        self.vocab.build_vocab(self.recipe_df[\"Instructions\"].tolist())\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.recipe_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_file = os.path.join(self.img_dir, self.recipe_df.iloc[index,4] + '.jpg')\n",
    "        img = Image.open(img_file)\n",
    "        img = img.convert('RGB')\n",
    "        img_name = self.recipe_df.iloc[index,1]\n",
    "        recipe = self.recipe_df.iloc[index,3]\n",
    "\n",
    "        recipe_tokens = []\n",
    "        recipe_tokens += [self.vocab.stoi['<START>']]\n",
    "        recipe_tokens += self.vocab.numericalize(str(recipe))\n",
    "        recipe_tokens += [self.vocab.stoi['<END>']] \n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(recipe_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = CustomDataset(recipe_csv='Food Ingredients and Recipe Dataset with Image Name Mapping.csv',\n",
    "                                    img_dir='Food Images', transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img,recipe_tokens = torch_data[10]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [torch_data.vocab.itos[token] for token in recipe_tokens.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_data_ref = pd.read_csv('Food Ingredients and Recipe Dataset with Image Name Mapping.csv')\n",
    "# image_data_ref.ndim\n",
    "# display(image_data_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet101(pretrained = True)\n",
    "# resnet.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.arch = models.resnet101(pretrained = True)\n",
    "        self.arch.fc = nn.Linear(self.arch.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.arch(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, recipe_tokens):\n",
    "        embeddings = self.dropout(self.embed(recipe_tokens))\n",
    "        #Add features as a dimension for embeddings as the 'first' input in the sequence\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, recipe_tokens):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, recipe_tokens)\n",
    "        return outputs\n",
    "\n",
    "    def recipe_generate(self, image, vocabulary, max_length=50):\n",
    "        result_recipe = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                \n",
    "                output_dist = output.data.view(-1).div(0.8).exp()\n",
    "                top_i = torch.multinomial(output_dist, 1)[0]\n",
    "                \n",
    "                result_recipe.append(top_i.item())\n",
    "\n",
    "                x = self.decoderRNN.embed(torch.unsqueeze(top_i, 0)).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[top_i.item()] == \"<END>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_recipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"Saving checkpoint ...\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"Loading checkpoint ... \")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step\n",
    "\n",
    "def train():\n",
    "    # train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "    #                                    transforms.RandomResizedCrop(224),\n",
    "    #                                    transforms.RandomHorizontalFlip(),\n",
    "    #                                    transforms.ToTensor(),\n",
    "    #                                    transforms.Normalize([0.485,0.456,0.406],\n",
    "    #                                                        [0.229,0.224,0.225])])\n",
    "    # valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "    #                                     transforms.CenterCrop(size=224),\n",
    "    #                                     transforms.ToTensor(),\n",
    "    #                                     transforms.Normalize([0.485,0.456,0.406],\n",
    "    #                                                         [0.229,0.224,0.225])])\n",
    "    # test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "    #                                     transforms.CenterCrop(size=224),\n",
    "    #                                     transforms.ToTensor(),\n",
    "    #                                     transforms.Normalize([0.485,0.456,0.406],\n",
    "    #                                                         [0.229,0.224,0.225])])\n",
    "\n",
    "    train_dls = torch.utils.data.DataLoader(torch_data, \n",
    "                                           batch_size=64, shuffle=False,\n",
    "                                           collate_fn = CollateFn(pad_idx=torch_data.vocab.stoi[\"<PAD>\"]))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    load_model = False\n",
    "    save_model = True\n",
    "    train_CNN = False\n",
    "\n",
    "    # Hyperparameters\n",
    "    embed_size = 256\n",
    "    hidden_size = 256\n",
    "    vocab_size = len(torch_data.vocab)\n",
    "    num_layers = 2\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 10\n",
    "\n",
    "   \n",
    "    step = 0\n",
    "\n",
    "    # initialize model, loss etc\n",
    "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=torch_data.vocab.stoi[\"<PAD>\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Only finetune the CNN\n",
    "    for param in model.encoderCNN.parameters():\n",
    "       param.requires_grad = False\n",
    "\n",
    "    if load_model:\n",
    "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Uncomment the line below to see a couple of test cases\n",
    "        # print_examples(model, device, dataset)\n",
    "\n",
    "        for idx, (imgs, recipe_tokens) in tqdm(\n",
    "            enumerate(train_dls), total=len(train_dls), leave=False\n",
    "        ):\n",
    "            imgs = imgs.to(device)\n",
    "            recipe_tokens = recipe_tokens.to(device)\n",
    "\n",
    "            outputs = model(imgs, recipe_tokens[:-1])\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]), recipe_tokens.reshape(-1)\n",
    "            )\n",
    "            if idx % 200 == 0:\n",
    "                print(\"Loss: \")\n",
    "                print(loss.item())\n",
    "            # writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "    \n",
    "    if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "            }\n",
    "            save_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\erice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'my_checkpoint.pth.tar'\n",
    "checkpoint = torch.load(path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(torch_data.vocab)\n",
    "num_layers = 2\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 1\n",
    "\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# No use\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']  # ALREADY DEFFINED ABOE\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,recipe_tokens = torch_data[1]\n",
    "# model.caption_image(img.unsqueeze(0), torch_data.vocab, max_length = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                             1\n",
       "Title                                    Crispy Salt and Pepper Potatoes\n",
       "Ingredients            ['2 large egg whites', '1 pound new potatoes (...\n",
       "Instructions           Preheat oven to 400°F and line a rimmed baking...\n",
       "Image_Name                    crispy-salt-and-pepper-potatoes-dan-kluger\n",
       "Cleaned_Ingredients    ['2 large egg whites', '1 pound new potatoes (...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_data.recipe_df.iloc[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
