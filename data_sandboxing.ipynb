{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import splitfolders\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer   # for tokenization\n",
    "from collections import Counter     # for tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitfolders.ratio(\"Food Images\", output=\"food_images\", \n",
    "#                    seed=30, ratio=(.7, .2, .1), \n",
    "#                    group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crispy-salt-and-pepper-potatoes-dan-kluger'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('Food Ingredients and Recipe Dataset with Image Name Mapping.csv')\n",
    "data_df.iloc[1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './food_images'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                           [0.229,0.224,0.225])])\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(size=224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                        [0.229,0.224,0.225])])\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(size=224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                        [0.229,0.224,0.225])])\n",
    "\n",
    "# train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "# valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "# test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(train_data,batch_size = 64, shuffle=True)\n",
    "# validloader = torch.utils.data.DataLoader(valid_data,batch_size = 64, shuffle=True)\n",
    "# testloader = torch.utils.data.DataLoader(test_data,batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq = 1):\n",
    "        self.itos = {0:'<PAD>',1:'<START>',2:'<END>',3:'<UNK>'}\n",
    "        self.stoi = {'<PAD>':0,'<START>':1,'<END>':2,'<UNK>':3}\n",
    "\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.frequencies = Counter()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            sentence_tokens = self.tokenizer(str(sentence))\n",
    "            self.frequencies.update(sentence_tokens)\n",
    "            #TODO: handle null sentences/instructions\n",
    "            for token in sentence_tokens:\n",
    "                if token not in self.stoi.keys() and self.frequencies[token] >= self.min_freq:\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx +=1\n",
    "\n",
    "    def numericalize(self, sentence):\n",
    "        sentence_tokens = self.tokenizer(str(sentence))\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi['<UNK>'] for token in sentence_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        recipes = [item[1] for item in batch]\n",
    "        recipes = pad_sequence(recipes, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, recipe_csv, transform = None, min_freq = 3):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        init_df = pd.read_csv(recipe_csv)\n",
    "        ###TODO cleaning function :\n",
    "        # clean_df = init_df.dropna(subset=['Instructions'], inplace=True)\n",
    "        clean_df = init_df[init_df[\"Image_Name\"] != '#NAME?']\n",
    "        \n",
    "        self.recipe_df = clean_df\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "        self.vocab = Vocab(min_freq)\n",
    "        self.vocab.build_vocab(self.recipe_df[\"Instructions\"].tolist())\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.recipe_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_file = os.path.join(self.img_dir, self.recipe_df.iloc[index,4] + '.jpg')\n",
    "        img = Image.open(img_file)\n",
    "        img = img.convert('RGB')\n",
    "        img_name = self.recipe_df.iloc[index,1]\n",
    "        recipe = self.recipe_df.iloc[index,3]\n",
    "\n",
    "        recipe_tokens = []\n",
    "        recipe_tokens += [self.vocab.stoi['<START>']]\n",
    "        recipe_tokens += self.vocab.numericalize(str(recipe))\n",
    "        recipe_tokens += [self.vocab.stoi['<END>']] \n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(recipe_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = CustomDataset(recipe_csv='Food Ingredients and Recipe Dataset with Image Name Mapping.csv',\n",
    "                                    img_dir='Food Images', transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6392,  0.7077,  0.8104,  ..., -0.2513, -0.5253, -0.6794],\n",
       "         [ 0.5707,  0.6392,  0.7419,  ..., -0.3369, -0.5767, -0.6965],\n",
       "         [ 0.3823,  0.4508,  0.5364,  ..., -0.6109, -0.6965, -0.7308],\n",
       "         ...,\n",
       "         [ 1.7694,  1.6667,  1.4954,  ...,  1.4440,  1.5297,  1.5810],\n",
       "         [ 1.7694,  1.6667,  1.5125,  ...,  1.2899,  1.5125,  1.6153],\n",
       "         [ 1.7694,  1.6667,  1.5125,  ...,  1.2385,  1.4954,  1.6324]],\n",
       "\n",
       "        [[ 1.1856,  1.2381,  1.3256,  ..., -1.1253, -1.2304, -1.2829],\n",
       "         [ 1.1155,  1.1681,  1.2731,  ..., -1.1604, -1.2304, -1.2654],\n",
       "         [ 0.9405,  0.9930,  1.0980,  ..., -1.2829, -1.2304, -1.1954],\n",
       "         ...,\n",
       "         [ 1.8508,  1.7458,  1.5707,  ...,  1.6933,  1.7633,  1.7983],\n",
       "         [ 1.8508,  1.7458,  1.5707,  ...,  1.5707,  1.7283,  1.8333],\n",
       "         [ 1.8508,  1.7458,  1.5707,  ...,  1.5182,  1.7108,  1.8333]],\n",
       "\n",
       "        [[-1.6999, -1.6302, -1.5256,  ..., -1.5081, -1.5256, -1.5430],\n",
       "         [-1.7173, -1.6476, -1.5604,  ..., -1.5256, -1.5256, -1.5256],\n",
       "         [-1.7347, -1.6999, -1.6476,  ..., -1.5604, -1.4907, -1.4559],\n",
       "         ...,\n",
       "         [ 2.1346,  2.0300,  1.8557,  ..., -0.1487,  0.0779,  0.2348],\n",
       "         [ 2.1346,  2.0300,  1.8383,  ..., -0.3927, -0.0441,  0.1476],\n",
       "         [ 2.1346,  2.0300,  1.8383,  ..., -0.4798, -0.0964,  0.1302]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,recipe_tokens = torch_data[10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'put',\n",
       " 'the',\n",
       " 'chipotle',\n",
       " 'peppers',\n",
       " 'and',\n",
       " 'adobo',\n",
       " 'sauce',\n",
       " 'in',\n",
       " 'a',\n",
       " 'small',\n",
       " 'food',\n",
       " 'processor',\n",
       " 'or',\n",
       " 'blender',\n",
       " 'and',\n",
       " 'blend',\n",
       " 'until',\n",
       " 'the',\n",
       " 'mixture',\n",
       " 'turns',\n",
       " 'into',\n",
       " 'a',\n",
       " 'smooth',\n",
       " 'purée',\n",
       " '.',\n",
       " 'set',\n",
       " 'aside',\n",
       " '.',\n",
       " 'the',\n",
       " 'chipotle',\n",
       " 'purée',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'in',\n",
       " 'advance',\n",
       " ',',\n",
       " 'stored',\n",
       " 'in',\n",
       " 'an',\n",
       " 'airtight',\n",
       " 'container',\n",
       " ',',\n",
       " 'and',\n",
       " 'refrigerated',\n",
       " 'for',\n",
       " 'up',\n",
       " 'to',\n",
       " '2',\n",
       " 'months',\n",
       " '.',\n",
       " 'on',\n",
       " 'a',\n",
       " 'cutting',\n",
       " 'board',\n",
       " ',',\n",
       " 'sprinkle',\n",
       " 'the',\n",
       " 'garlic',\n",
       " 'with',\n",
       " 'a',\n",
       " 'large',\n",
       " 'pinch',\n",
       " 'of',\n",
       " 'salt',\n",
       " 'and',\n",
       " 'gather',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'small',\n",
       " 'mound',\n",
       " '.',\n",
       " 'holding',\n",
       " 'the',\n",
       " 'blunt',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'knife',\n",
       " 'with',\n",
       " 'both',\n",
       " 'hands',\n",
       " ',',\n",
       " 'press',\n",
       " 'and',\n",
       " 'scrape',\n",
       " 'the',\n",
       " '<UNK>',\n",
       " 'sharp',\n",
       " 'end',\n",
       " ',',\n",
       " 'holding',\n",
       " 'it',\n",
       " 'at',\n",
       " 'a',\n",
       " 'slight',\n",
       " 'angle',\n",
       " ',',\n",
       " 'across',\n",
       " 'the',\n",
       " 'garlic',\n",
       " 'mound',\n",
       " 'to',\n",
       " 'flatten',\n",
       " 'it',\n",
       " '.',\n",
       " 'repeat',\n",
       " ',',\n",
       " 'dragging',\n",
       " 'it',\n",
       " 'across',\n",
       " 'the',\n",
       " 'garlic',\n",
       " ',',\n",
       " 'until',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'smooth',\n",
       " 'paste',\n",
       " '.',\n",
       " 'set',\n",
       " 'aside',\n",
       " '.',\n",
       " 'in',\n",
       " 'a',\n",
       " 'small',\n",
       " 'bowl',\n",
       " ',',\n",
       " 'mix',\n",
       " 'the',\n",
       " 'cornstarch',\n",
       " 'and',\n",
       " '1½',\n",
       " 'tsp',\n",
       " '.',\n",
       " 'of',\n",
       " 'the',\n",
       " 'evaporated',\n",
       " 'milk',\n",
       " 'into',\n",
       " 'a',\n",
       " 'slurry',\n",
       " '.',\n",
       " 'pour',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'evaporated',\n",
       " 'milk',\n",
       " 'into',\n",
       " 'a',\n",
       " 'medium',\n",
       " 'saucepan',\n",
       " 'and',\n",
       " 'stir',\n",
       " 'in',\n",
       " 'the',\n",
       " 'slurry',\n",
       " '.',\n",
       " 'bring',\n",
       " 'to',\n",
       " 'a',\n",
       " 'boil',\n",
       " 'over',\n",
       " 'medium-high',\n",
       " 'heat',\n",
       " ',',\n",
       " 'whisking',\n",
       " 'constantly',\n",
       " '.',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'heat',\n",
       " 'to',\n",
       " 'low',\n",
       " 'and',\n",
       " 'add',\n",
       " 'the',\n",
       " 'cheddar',\n",
       " 'gradually',\n",
       " 'by',\n",
       " 'the',\n",
       " 'handful',\n",
       " ',',\n",
       " 'stirring',\n",
       " 'until',\n",
       " 'the',\n",
       " 'cheddar',\n",
       " 'is',\n",
       " 'melted',\n",
       " 'and',\n",
       " 'the',\n",
       " 'mixture',\n",
       " 'is',\n",
       " 'smooth',\n",
       " '.',\n",
       " 'add',\n",
       " 'the',\n",
       " 'cream',\n",
       " 'cheese',\n",
       " 'and',\n",
       " 'whisk',\n",
       " 'until',\n",
       " 'it',\n",
       " 'melts',\n",
       " '.',\n",
       " 'stir',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mayonnaise',\n",
       " ',',\n",
       " 'pimento',\n",
       " 'peppers',\n",
       " ',',\n",
       " '1½',\n",
       " 'tsp',\n",
       " '.',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chipotle',\n",
       " 'purée',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'garlic',\n",
       " 'paste',\n",
       " '.',\n",
       " 'season',\n",
       " 'with',\n",
       " 'salt',\n",
       " '.',\n",
       " 'transfer',\n",
       " 'to',\n",
       " 'a',\n",
       " 'serving',\n",
       " 'bowl',\n",
       " 'or',\n",
       " 'keep',\n",
       " 'it',\n",
       " 'warm',\n",
       " 'in',\n",
       " 'a',\n",
       " 'slow',\n",
       " 'cooker',\n",
       " 'and',\n",
       " 'serve',\n",
       " 'immediately',\n",
       " '.',\n",
       " 'to',\n",
       " 'reheat',\n",
       " 'the',\n",
       " 'sauce',\n",
       " ',',\n",
       " 'microwave',\n",
       " 'it',\n",
       " ',',\n",
       " 'stirring',\n",
       " 'every',\n",
       " '30',\n",
       " 'seconds',\n",
       " ',',\n",
       " 'until',\n",
       " 'fully',\n",
       " 'melted',\n",
       " '.',\n",
       " '<END>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch_data.vocab.itos[token] for token in recipe_tokens.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "      <td>Pat chicken dry with paper towels, season all ...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "      <td>Preheat oven to 400°F and line a rimmed baking...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "      <td>Place a rack in middle of oven; preheat to 400...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "      <td>Preheat oven to 350°F with rack in middle. Gen...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "      <td>Stir together brown sugar and hot water in a c...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>13496</td>\n",
       "      <td>Brownie Pudding Cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "      <td>Preheat the oven to 350°F. Into a bowl sift to...</td>\n",
       "      <td>brownie-pudding-cake-14408</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>13497</td>\n",
       "      <td>Israeli Couscous with Roasted Butternut Squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "      <td>Preheat oven to 475°F.\\nHalve lemons and scoop...</td>\n",
       "      <td>israeli-couscous-with-roasted-butternut-squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>13498</td>\n",
       "      <td>Rice with Soy-Glazed Bonito Flakes and Sesame ...</td>\n",
       "      <td>['Leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "      <td>If using katsuo bushi flakes from package, moi...</td>\n",
       "      <td>rice-with-soy-glazed-bonito-flakes-and-sesame-...</td>\n",
       "      <td>['Leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>13499</td>\n",
       "      <td>Spanakopita</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "      <td>Melt 1 tablespoon butter in a 12-inch heavy sk...</td>\n",
       "      <td>spanakopita-107344</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>13500</td>\n",
       "      <td>Mexican Poblano, Spinach, and Black Bean \"Lasa...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "      <td>Lay 4 chiles on their sides on racks of gas bu...</td>\n",
       "      <td>mexican-poblano-spinach-and-black-bean-lasagne...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13501 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "0               0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1               1                    Crispy Salt and Pepper Potatoes   \n",
       "2               2                        Thanksgiving Mac and Cheese   \n",
       "3               3                 Italian Sausage and Bread Stuffing   \n",
       "4               4                                       Newton's Law   \n",
       "...           ...                                                ...   \n",
       "13496       13496                               Brownie Pudding Cake   \n",
       "13497       13497  Israeli Couscous with Roasted Butternut Squash...   \n",
       "13498       13498  Rice with Soy-Glazed Bonito Flakes and Sesame ...   \n",
       "13499       13499                                        Spanakopita   \n",
       "13500       13500  Mexican Poblano, Spinach, and Black Bean \"Lasa...   \n",
       "\n",
       "                                             Ingredients  \\\n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...   \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
       "3      ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
       "...                                                  ...   \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...   \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...   \n",
       "13498  ['Leftover katsuo bushi (dried bonito flakes) ...   \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...   \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...   \n",
       "\n",
       "                                            Instructions  \\\n",
       "0      Pat chicken dry with paper towels, season all ...   \n",
       "1      Preheat oven to 400°F and line a rimmed baking...   \n",
       "2      Place a rack in middle of oven; preheat to 400...   \n",
       "3      Preheat oven to 350°F with rack in middle. Gen...   \n",
       "4      Stir together brown sugar and hot water in a c...   \n",
       "...                                                  ...   \n",
       "13496  Preheat the oven to 350°F. Into a bowl sift to...   \n",
       "13497  Preheat oven to 475°F.\\nHalve lemons and scoop...   \n",
       "13498  If using katsuo bushi flakes from package, moi...   \n",
       "13499  Melt 1 tablespoon butter in a 12-inch heavy sk...   \n",
       "13500  Lay 4 chiles on their sides on racks of gas bu...   \n",
       "\n",
       "                                              Image_Name  \\\n",
       "0      miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1             crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2             thanksgiving-mac-and-cheese-erick-williams   \n",
       "3              italian-sausage-and-bread-stuffing-240559   \n",
       "4                     newtons-law-apple-bourbon-cocktail   \n",
       "...                                                  ...   \n",
       "13496                         brownie-pudding-cake-14408   \n",
       "13497  israeli-couscous-with-roasted-butternut-squash...   \n",
       "13498  rice-with-soy-glazed-bonito-flakes-and-sesame-...   \n",
       "13499                                 spanakopita-107344   \n",
       "13500  mexican-poblano-spinach-and-black-bean-lasagne...   \n",
       "\n",
       "                                     Cleaned_Ingredients  \n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...  \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
       "3      ['1 (¾- to 1-pound) round Italian loaf, cut in...  \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n",
       "...                                                  ...  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...  \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...  \n",
       "13498  ['Leftover katsuo bushi (dried bonito flakes) ...  \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...  \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...  \n",
       "\n",
       "[13501 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_data_ref = pd.read_csv('Food Ingredients and Recipe Dataset with Image Name Mapping.csv')\n",
    "image_data_ref.ndim\n",
    "display(image_data_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\erice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet101(pretrained = True)\n",
    "resnet.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.arch = models.resnet101(pretrained = True)\n",
    "        self.arch.fc = nn.Linear(self.arch.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.arch(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, recipe_tokens):\n",
    "        embeddings = self.dropout(self.embed(recipe_tokens))\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<END>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step\n",
    "\n",
    "def train():\n",
    "    # train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "    #                                    transforms.RandomResizedCrop(224),\n",
    "    #                                    transforms.RandomHorizontalFlip(),\n",
    "    #                                    transforms.ToTensor(),\n",
    "    #                                    transforms.Normalize([0.485,0.456,0.406],\n",
    "    #                                                        [0.229,0.224,0.225])])\n",
    "    # valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "    #                                     transforms.CenterCrop(size=224),\n",
    "    #                                     transforms.ToTensor(),\n",
    "    #                                     transforms.Normalize([0.485,0.456,0.406],\n",
    "    #                                                         [0.229,0.224,0.225])])\n",
    "    # test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "    #                                     transforms.CenterCrop(size=224),\n",
    "    #                                     transforms.ToTensor(),\n",
    "    #                                     transforms.Normalize([0.485,0.456,0.406],\n",
    "    #                                                         [0.229,0.224,0.225])])\n",
    "\n",
    "    train_dls = torch.utils.data.DataLoader(torch_data, \n",
    "                                           batch_size=64, shuffle=False,\n",
    "                                           collate_fn = CollateFn(pad_idx=torch_data.vocab.stoi[\"<PAD>\"]))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    load_model = False\n",
    "    save_model = True\n",
    "    train_CNN = False\n",
    "\n",
    "    # Hyperparameters\n",
    "    embed_size = 256\n",
    "    hidden_size = 256\n",
    "    vocab_size = len(torch_data.vocab)\n",
    "    num_layers = 1\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 10\n",
    "\n",
    "   \n",
    "    step = 0\n",
    "\n",
    "    # initialize model, loss etc\n",
    "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=torch_data.vocab.stoi[\"<PAD>\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Only finetune the CNN\n",
    "    for param in model.encoderCNN.parameters():\n",
    "       param.requires_grad = False\n",
    "\n",
    "    if load_model:\n",
    "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Uncomment the line below to see a couple of test cases\n",
    "        # print_examples(model, device, dataset)\n",
    "\n",
    "        for idx, (imgs, captions) in tqdm(\n",
    "            enumerate(train_dls), total=len(train_dls), leave=False\n",
    "        ):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "            )\n",
    "\n",
    "            # writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "    \n",
    "    if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "            }\n",
    "            save_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\erice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'my_checkpoint.pth.tar'\n",
    "checkpoint = torch.load(path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(torch_data.vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 1\n",
    "\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# No use\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']  # ALREADY DEFFINED ABOE\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erice\\AppData\\Local\\Temp\\ipykernel_16292\\552917542.py:4: RuntimeWarning: invalid value encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m img,recipe_tokens \u001b[38;5;241m=\u001b[39m torch_data[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaption_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36mCNNtoRNN.caption_image\u001b[1;34m(self, image, vocabulary, max_length)\u001b[0m\n\u001b[0;32m     29\u001b[0m hiddens, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderRNN\u001b[38;5;241m.\u001b[39mlstm(x, states)\n\u001b[0;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderRNN\u001b[38;5;241m.\u001b[39mlinear(hiddens\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m result_caption\u001b[38;5;241m.\u001b[39mappend(predicted\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted)\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36msample\u001b[1;34m(preds, temperature)\u001b[0m\n\u001b[0;32m      5\u001b[0m exp_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(preds)\n\u001b[0;32m      6\u001b[0m preds \u001b[38;5;241m=\u001b[39m exp_preds \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_preds)\n\u001b[1;32m----> 7\u001b[0m probas \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(probas)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "img,recipe_tokens = torch_data[1]\n",
    "model.caption_image(img.unsqueeze(0), torch_data.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1187,  1.1358,  1.3927,  ...,  0.9988,  1.7865,  1.8208],\n",
       "         [ 1.1187,  1.1358,  1.3927,  ...,  0.9988,  1.7865,  1.8208],\n",
       "         [ 1.0673,  1.0844,  1.3413,  ...,  0.7077,  1.3755,  1.4098],\n",
       "         ...,\n",
       "         [-0.9877, -0.9877, -0.8507,  ..., -0.5767, -0.8507, -0.8678],\n",
       "         [-0.7822, -0.7822, -0.6623,  ..., -0.6281, -0.9363, -0.9534],\n",
       "         [-0.7822, -0.7822, -0.6623,  ..., -0.6281, -0.9363, -0.9534]],\n",
       "\n",
       "        [[ 0.9055,  0.9230,  1.2206,  ...,  0.3102,  1.0280,  1.0630],\n",
       "         [ 0.9055,  0.9230,  1.2206,  ...,  0.3102,  1.0280,  1.0630],\n",
       "         [ 0.8354,  0.8529,  1.1506,  ...,  0.0476,  0.6429,  0.6779],\n",
       "         ...,\n",
       "         [-0.4076, -0.4076, -0.2500,  ...,  0.0476, -0.1975, -0.2150],\n",
       "         [-0.1800, -0.1800, -0.0399,  ..., -0.0224, -0.3025, -0.3200],\n",
       "         [-0.1800, -0.1800, -0.0399,  ..., -0.0224, -0.3025, -0.3200]],\n",
       "\n",
       "        [[ 0.8274,  0.8448,  1.1585,  ..., -0.0790,  0.6531,  0.6879],\n",
       "         [ 0.8274,  0.8448,  1.1585,  ..., -0.0790,  0.6531,  0.6879],\n",
       "         [ 0.7576,  0.7751,  1.0888,  ..., -0.3055,  0.3045,  0.3393],\n",
       "         ...,\n",
       "         [-0.9853, -0.9853, -0.9504,  ...,  0.0605, -0.1835, -0.2010],\n",
       "         [-0.8110, -0.8110, -0.8110,  ..., -0.0092, -0.2881, -0.3055],\n",
       "         [-0.8110, -0.8110, -0.8110,  ..., -0.0092, -0.2881, -0.3055]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
